{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAVwcH0oFVST"
   },
   "source": [
    "# M3. Computer Vision (24.05.2024), Dr. Aygul Zagidullina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0EAsBWDFQTA"
   },
   "source": [
    "# Intuitions on Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgdEUO4JFQTB"
   },
   "source": [
    "Let's build some intuitions about the **`convolution operations`**.\n",
    "\n",
    "üéØ <b><u>Goals</u>:</b>\n",
    "- **Understand convolution operations**\n",
    "- **Visualize**\n",
    "    - convolution kernels\n",
    "    - the effects of a convolution kernel applied on images\n",
    "\n",
    "<hr>\n",
    "\n",
    "üñº <b><u>Convolutional Neural Networks are Neural Networks specifically designed to work on images</u></b>.\n",
    "\n",
    "- üßÆ This is made possible thanks to **`convolution operations`**.\n",
    "\n",
    "- üîé These specific mathematical operations apply a **`filter`** (i.e. a set of **`kernels`**, one per channel) to an input image and create an **`output representation`**. For Convolutional Neural Networks, this can also be called:\n",
    "    * a **`\"convoluted representation/feature\"`**,\n",
    "    * or a **`\"convolution\"`**,\n",
    "    * or also an **`\"activation\"`** (as it corresponds to the activation of a given layer).\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/CNN/convolution.png\" width=\"300\">\n",
    "\n",
    "---\n",
    "\n",
    "‚ùóÔ∏è <b><u>Remarks</u></b> ‚ùóÔ∏è\n",
    "\n",
    "* It is important to understand that **the same kernel (i.e. the same weights) is applied to different areas of the images**.\n",
    "\n",
    "* This is completely different from Dense Neural Networks:\n",
    "    * In `Dense/\"Fully Connected\" Neural Networks`, each weight of a given neuron is related to only one input coordinate (which, in images, would correspond to one pixel).\n",
    "    * In `Convolution Neural Networks`, the weights of a kernel are not applied to only one feature input, i.e. one pixel, but to different pixels, \"step by step\"!\n",
    "\n",
    "üëâ You can think of each kernel (or each filter in the case of colored images) as a **`magnifying glass`** through which you see the image. Similarly to your eyes, kernels cannot capture everything in a picture at once, but they ***scan different parts of a picture to understand the whole picture that is being analyzed***.\n",
    "\n",
    "üé¨ So let's have a closer look at `convolution operations`, and their impact in `Convolutional Neural Networks`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNxhogCbFQTC"
   },
   "source": [
    "## (0) The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'circles-and-triangles.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcircles-and-triangles.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m      4\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall()\n",
      "File \u001b[1;32mc:\\Users\\rotsc\\anaconda3\\envs\\Py9\\lib\\zipfile.py:1248\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'circles-and-triangles.zip'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"circles-and-triangles.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHvm3q91FQTC"
   },
   "source": [
    "Let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yteL9pifFQTC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"unzip\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!unzip -q circles-and-triangles.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0SaWoOrFQTD"
   },
   "source": [
    "First, let's use the following function `load_data` to... load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdP7Iz0qFQTD"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\circles\\\\circle_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mc)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m---> 32\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_circles):\n\u001b[0;32m     17\u001b[0m     c_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircles\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircle_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_path\u001b[49m\u001b[43m)\u001b[49m)[:, :, :\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_triangles):\n",
      "File \u001b[1;32mc:\\Users\\rotsc\\anaconda3\\envs\\Py9\\lib\\site-packages\\PIL\\Image.py:3505\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3502\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3505\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\circles\\\\circle_0.png'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_data(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        X.append(np.array(Image.open(c_path))[:, :, :1])\n",
    "        y.append(0)\n",
    "\n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        X.append(np.array(Image.open(t_path))[:, :, :1])\n",
    "        y.append(1)\n",
    "\n",
    "    c = list(zip(X, y))\n",
    "    np.random.shuffle(c)\n",
    "    X, y = zip(*c)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = load_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdiCi9elFQTD"
   },
   "source": [
    "‚ùì **Question about the shape** ‚ùì\n",
    "\n",
    "* How many images do we have?\n",
    "* What are their dimensions?\n",
    "* Can we comment on the number of channels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716478473501,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "MJCYoAqrFQTD",
    "outputId": "985ea724-bd21-4707-fe50-66adc4c5984d",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s06NW6H9FQTE"
   },
   "source": [
    "<details>\n",
    "    <summary><i>About the number of channels:</i></summary>\n",
    "       \n",
    "  We need only one channel to compute the \"*blackness intensity*\" of a pixel with 0 corresponding to a black pixel and 1 corresponding to a white pixel. The last dimension corresponds to some kind of  \"Black to white channel\".\n",
    "        \n",
    "üé® For colored images, the last dimension would be equal to 3 for `Red, Green, Blue (RGB)`\n",
    "\n",
    "üëâ Have fun playing with the intensities of Red, Green and Blue <a href=\"https://www.w3schools.com/colors/colors_rgb.asp\">`here`</a>\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROHEbdjpFQTE"
   },
   "source": [
    "‚ùì **Question about the normalization** ‚ùì\n",
    "\n",
    "Do these images need some normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1716478776253,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "WjJfoJZQFQTE",
    "outputId": "2823600b-2466-4ae2-fac3-4657a7df704d",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "np.min(X), np.max(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7fu3DCSFQTF"
   },
   "source": [
    "üëÄ Let's have a look at some images with `plt.imshow` and show their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2499,
     "status": "ok",
     "timestamp": 1716478475996,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "jpPC34d3FQTF",
    "outputId": "9e7f97ea-423e-4fa8-d807-5a98ca9556d7"
   },
   "outputs": [],
   "source": [
    "for iter_, (image, label) in enumerate(zip(X, y)):\n",
    "    plt.imshow(image[:, :, 0], cmap='gray')\n",
    "    plt.title('Triangle' if label == 1 else 'Circle')\n",
    "    plt.show()\n",
    "\n",
    "    if iter_ > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bNanlgyFQTF"
   },
   "source": [
    "‚ùì **Question: How many classes are we going to predict** ‚ùì\n",
    "\n",
    "_This information will help us design the last layer of your Convolutional Network_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716478475996,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "IPGR2FCzFQTF",
    "outputId": "05f753f5-af55-4415-b8c9-ae6d3e1bc29c",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "np.unique(y, return_counts = True)\n",
    "# Two categories to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hE720RNFQTF",
    "tags": [
     "delete"
    ]
   },
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "        \n",
    "- We have only two categories: Triangle (1) and Circle (0).  \n",
    "- Hence, it is a binary classification and we will need only one single neuron in the predictive layer.\n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1atohTc0FQTF"
   },
   "source": [
    "## (1) Kernels & Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHvdD3H1FQTF"
   },
   "source": [
    "üéÅ The following function **`compute_convolution`** performs a **convolution operation** $ \\Leftrightarrow $ i.e. *it applies a kernel to an image*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZzp2w-5FQTF"
   },
   "source": [
    "‚ùì **Question about the `compute_convolution` function** ‚ùì\n",
    "\n",
    "Run it and try to understand the different steps of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvV-js9RFQTG"
   },
   "outputs": [],
   "source": [
    "def compute_convolution(input_image, kernel):\n",
    "    # Parameters\n",
    "    kernel = np.array(kernel)\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    img = np.squeeze(input_image) # Removes dimensions of size 1\n",
    "    img_height, img_width = img.shape\n",
    "\n",
    "    output_image = []\n",
    "\n",
    "    for x in range(img_height - kernel_height + 1):\n",
    "        arr = []\n",
    "\n",
    "        for y in range(img_width - kernel_width + 1):\n",
    "\n",
    "            a = np.multiply(img[x: x + kernel_height, y: y + kernel_width],\n",
    "                            kernel)\n",
    "            arr.append(a.sum())\n",
    "\n",
    "        output_image.append(arr)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFjG2EtjFQTG"
   },
   "source": [
    "‚ùì **How does the `compute_convolution` function work in practice** ‚ùì\n",
    "\n",
    "1. Choose any image from the input dataset\n",
    "2. Apply the `identity_kernel` to it\n",
    "3. Display both the input image and the output image.\n",
    "4. Do you see any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpCZiGw3FQTG"
   },
   "outputs": [],
   "source": [
    "identity_kernel = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1716478476929,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "IEU6YbeBFQTG",
    "outputId": "c0c9399f-ac13-4838-b5b0-af4c6449f11c",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "input_image = X[0]\n",
    "kernel = identity_kernel\n",
    "\n",
    "output_image = compute_convolution(input_image, kernel)\n",
    "\n",
    "# Original Image\n",
    "plt.imshow(np.squeeze(X[0]), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Convoluted image\n",
    "plt.imshow(output_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWjWnzeAFQTG"
   },
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "        \n",
    "üßëüèª‚Äçüè´ The previous kernel corresponds to the **`identity_kernel`**, meaning that ***the output is equal to the input***...\n",
    "    \n",
    "üïµüèª‚Äç‚ôÇÔ∏è It basically did nothing to the input image. It you think about it thoroughly, that's not surprising. With this kernel, only the pixel scanned in the middle was kept and multiplied by one, the surrounding pixels were multiplied by zero.        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCbJilJLFQTG"
   },
   "source": [
    "üéÅThe function **`plot_convolution`** plots the output image after applying a kernel to an input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDoMgJhTFQTG"
   },
   "outputs": [],
   "source": [
    "def plot_convolution(img, kernel, activation=False):\n",
    "    ''' The following printing function ease the visualization'''\n",
    "\n",
    "    img = np.squeeze(img)\n",
    "    output_img = compute_convolution(img, kernel)\n",
    "    if activation:\n",
    "        output_img = np.maximum(output_img, 0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax1 = plt.subplot2grid((3,3),(0,0), rowspan=3)\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.title.set_text('Input image')\n",
    "\n",
    "    ax2 = plt.subplot2grid((3,3),(1, 1))\n",
    "    ax2.imshow(kernel, cmap='gray')\n",
    "    ax2.title.set_text('Kernel')\n",
    "\n",
    "    ax3 = plt.subplot2grid((3,3),(0, 2), rowspan=3)\n",
    "    ax3.imshow(output_img, cmap='gray')\n",
    "    ax3.title.set_text('Output image')\n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqfNj4ftFQTG"
   },
   "source": [
    "‚ùì **Question** ‚ùì Apply `plot_convolution` with the following `kernel_1` once on an triangle and once on a circle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-jXMcy6FQTH"
   },
   "outputs": [],
   "source": [
    "kernel_1 = [\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [-1, -1, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1716478478420,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "Coq-QranFQTH",
    "outputId": "4b5bc8ad-4c14-4e79-9903-6deb6024051e",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "plot_convolution(X[0], kernel_1)\n",
    "plot_convolution(X[5], kernel_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrQkDg8aFQTH"
   },
   "source": [
    "üëÜ Let's analyze what just happened:\n",
    "\n",
    "* White pixels correspond to high values and black pixels to low values.\n",
    "* In a Neural Network, remember that we use activation functions to remove linearities.\n",
    "    * *For example*, when the activation function is `relu`, you already know that it simply corresponds to setting the negative values to 0.\n",
    "\n",
    "---\n",
    "\n",
    "‚ùì **What is the impact of the activation function in a Convolutional Layer ?** ‚ùì\n",
    "\n",
    "Re-run the previous function `plot_convolution` with `activation` set to `True` (in this case, the activation function _is_ the relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1716478479850,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "Rnwx0jtPFQTH",
    "outputId": "2720ca53-a318-4999-cc76-e3c7f77f6e85",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "plot_convolution(X[0], kernel_1, activation=True)\n",
    "plot_convolution(X[5], kernel_1, activation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDD-a73QFQTH"
   },
   "source": [
    "üëÜ This kernel is actually highlighting the edges in a given direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSMOvJsrFQTH"
   },
   "source": [
    "‚ùì **Play with different kernels...** ‚ùì\n",
    "\n",
    "Try the following kernels to check the different edges they can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfqPf9SBFQTH"
   },
   "outputs": [],
   "source": [
    "kernel_2 = [\n",
    "    [-1, -1, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "kernel_3 = [\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "]\n",
    "\n",
    "kernel_4 = [\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "executionInfo": {
     "elapsed": 1772,
     "status": "ok",
     "timestamp": 1716478482109,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "vVikis5bFQTI",
    "outputId": "1e5344da-aafc-4b23-d1ca-654a0a96e79e",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "plot_convolution(X[0], kernel_2, activation=True)\n",
    "plot_convolution(X[0], kernel_3, activation=True)\n",
    "plot_convolution(X[0], kernel_4, activation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4A1YobQFQTI"
   },
   "source": [
    "‚ùì **What is the effect of the kernel size** ‚ùì\n",
    "\n",
    "Try the _kernel_big_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716478482110,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "KH1i0fY1FQTI",
    "outputId": "a6c89059-7584-4210-ddea-ae821a53b9dc"
   },
   "outputs": [],
   "source": [
    "kernel_big = np.array([\n",
    "    np.ones((10,)),\n",
    "    np.ones((10,)),\n",
    "    np.ones((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.zeros((10,)),\n",
    "    np.ones((10,))*-1,\n",
    "    np.ones((10,))*-1,\n",
    "    np.ones((10,))*-1,\n",
    "])\n",
    "\n",
    "kernel_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1716478483319,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "UGYQSfUaFQTI",
    "outputId": "e09bcaa2-8edc-4431-fba9-041d5439e014"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_convolution(X[0], kernel_big, activation=True)\n",
    "plot_convolution(X[5], kernel_big, activation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9hISg9wFQTL"
   },
   "source": [
    "‚ùì **Try another kernel**  ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1716478484614,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "7NZLEN8FFQTM",
    "outputId": "9184d90b-38a3-4126-807d-d168b35f8c78"
   },
   "outputs": [],
   "source": [
    "random_kernel = np.random.uniform(-10, 10, (5, 5))\n",
    "\n",
    "plot_convolution(X[0], random_kernel, activation=True)\n",
    "plot_convolution(X[5], random_kernel, activation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So_88OzMFQTM"
   },
   "source": [
    "Now that you've got the idea of what a convolution operation does to an image, let's see how it goes with a \"real\" Convolutional Neural Network. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g5H0LW6FQTM"
   },
   "source": [
    "## (2) Training a CNN to detect triangles and circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hRxGoHoFQTM"
   },
   "source": [
    "### (2.1) Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKr1JCQWFQTM"
   },
   "source": [
    "\n",
    "‚ùì **Question: Designing a CNN** ‚ùì\n",
    "\n",
    "Write a Convolutional Network that has\n",
    "- a Convolutional Layer with 16 filters with $ (4, 4) $ kernels.\n",
    "- a Convolutional Layer with 32 filters with $ (3, 3) $ kernels.\n",
    "- a Convolutional Layer with 64 filters with $ (3, 3) $ kernels.\n",
    "- a Convolutional Layer with 64 filters with $ (2, 2) $ kernels.\n",
    "\n",
    "with:\n",
    "- A Max-Pooling Layer (with a $ (2, 2) $ pool-size) after each convolution.\n",
    "- A Hidden Dense Layer with the size of your choice, be reasonable:\n",
    "    - after the flattening part\n",
    "    - but before the last layer\n",
    "\n",
    "\n",
    "Also, make sure to compile your model with the appropriate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZEABuV4FQTM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(16, (4, 4), input_shape = (76, 78, 1), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (2, 2), activation = 'relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units = 10, activation = 'relu'))\n",
    "    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics =['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmbNF23NFQTM"
   },
   "source": [
    "### (2.2) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1716478951130,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "eh9rveUFFQTM",
    "outputId": "742f2286-86d8-40b4-eda6-a262e8c6d560",
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716478951130,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "5_QVtxvZFQTN",
    "outputId": "01634009-3e0c-442b-a368-3660e8d610a1",
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters on the first Conv2D\n",
    "( (4*4) * 1 + 1) * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716478951131,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "ALdBoNFyFQTN",
    "outputId": "169b0179-2ac9-4a91-b3f6-fe031f349f09",
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters on the second Conv2D\n",
    "( (3*3) * 16 + 1) * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716478951131,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "Gi2phnvTFQTN",
    "outputId": "f7420100-073b-4984-da89-bd58ea295e98",
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters on the third Conv2D\n",
    "( (3*3) * 32 + 1) * 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716478951131,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "3XCjq8TTFQTN",
    "outputId": "f14c333f-34b0-456a-aea5-be1607b4fcb9",
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters on the fourth and last Conv2D\n",
    "( (2*2) * 64 + 1) * 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npeHwXELFQTN"
   },
   "source": [
    "‚ùì **Question: Training the CNN** ‚ùì\n",
    "\n",
    "* Fit the model. We achieve an accuracy of *at least* 90%.\n",
    "\n",
    "    * When you reach such a high score, it may sound suspicious and you would probably ask yourself whether the model is overfitting or not... but let's ignore it for this exercise üòè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11524,
     "status": "ok",
     "timestamp": 1716478962653,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "gVQy7z_qFQTN",
    "outputId": "8e4dda71-5316-4c6d-b760-fe37e1d7516e",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5)\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                    validation_split = 0.3,\n",
    "                    batch_size = 16,\n",
    "                    epochs=50,\n",
    "                    callbacks=[es],\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYRmdDxLFQTO"
   },
   "source": [
    "üéÅ We have the `plot_loss_accuracy` function.\n",
    "\n",
    "‚ùì **Question: does the CNN converge** ‚ùì\n",
    "\n",
    "_Also, do you see any sign of overfitting?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rkoVzP_FQTO"
   },
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "\n",
    "    # --- LOSS ---\n",
    "\n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    # --- ACCURACY\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'])\n",
    "    ax[1].plot(history.history['val_accuracy'])\n",
    "    ax[1].set_title('Model Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1716478963349,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "bB5kP6f3FQTO",
    "outputId": "e1f12574-1442-490c-80cf-ab7898baaae9",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)\n",
    "\n",
    "print(\"We are not overfitting ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te55gjwgFQTO"
   },
   "source": [
    "### üéÅ üìö (2.3) Deeper understanding of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3IdByB6FQTO"
   },
   "source": [
    "üë©üèª‚Äçüè´ For any Sequential Neural Network (Dense or Convolutional), you can:\n",
    "- print the **`.summary()`** to display the layers and the number of weights/parameters involved\n",
    "- access the different **`.layers`** of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1716478963349,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "4HYTxiSzFQTO",
    "outputId": "047b42b4-a33e-4584-c341-6cbac2d752c5"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "v8iBsx-YFQTO",
    "outputId": "335b2745-18ce-406d-b1c1-64533a613690"
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZV3z_XqFQTP"
   },
   "source": [
    " With the following table, we will have a better overview of the different weights, kernels and filters involved in the CNN you've built earlier:\n",
    "\n",
    "| layer_number | convolution_layer | kernel_number | channel_number |\n",
    "|--------------|-------------------|---------------|----------------|\n",
    "| 0            | conv2D no 1       | 16            | 1              |\n",
    "| 2            | conv2D no 2       | 32            | 16             |\n",
    "| 4            | conv2D no 3       | 64            | 32             |\n",
    "| 6            | conv2D no 4       | 64            | 64             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCTt457DFQTP"
   },
   "source": [
    "#### (2.3.1) Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRlK3iTjFQTP"
   },
   "source": [
    "üßëüèª‚Äçüè´ It is possible to **retrieve the values of all the kernels for each layer after training a CNN**.\n",
    "\n",
    "üëâ Let's focus first on the different parameters (**`.weights`**) of the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "U7fpSXmiFQTP",
    "outputId": "06a287b3-6c1b-46df-ac58-1487db2a59e3"
   },
   "outputs": [],
   "source": [
    "# Accessing the first convolutional layer of the CNN\n",
    "first_convolutional_layer = model.layers[0]\n",
    "first_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "irrPsFmkFQTP",
    "outputId": "6f39e171-8c6a-4d28-e7b7-de14eb551ae2"
   },
   "outputs": [],
   "source": [
    "# Weights of the first convolutional layer of the CNN - which was trained/optimized\n",
    "first_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hK4GN-vFQTP"
   },
   "source": [
    "üëÜ `TensorShape[4, 4, 1, 16]` represents:\n",
    "- the weights of each kernel (size `4` $\\times$ `4`)\n",
    "- there was only `1` channel (single B&W input)\n",
    "- and we have decided to apply `16` different kernels in this layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "ANg9wplYFQTP",
    "outputId": "2d5b966f-383a-49d7-f139-75bf0264c2c9"
   },
   "outputs": [],
   "source": [
    "# Biases of the first convolutional layer of the CNN - which was trained/optimized\n",
    "first_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Nm6w0tFQTQ"
   },
   "source": [
    "üëÜ Let's not forget the biases, one per new channel in the output image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "uxSQHoSbFQTQ",
    "outputId": "06451be8-658a-4235-cf3f-29a3eb53ce02"
   },
   "outputs": [],
   "source": [
    "# Notice that we have indeed 256 weights + 16 biases = 272 parameters for the first convolutional layer\n",
    "4*4*1*16+16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZL-2sCYFQTQ",
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "üëâ What are the parameters for the other layers of this Network ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "xmwKDVRnFQTQ",
    "outputId": "b9681250-589f-4fd7-f9b3-b6a4c31d39b7"
   },
   "outputs": [],
   "source": [
    "# Accessing the second convolutional layer of the CNN\n",
    "second_convolutional_layer = model.layers[2]\n",
    "second_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716478963350,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "kPQCLN1eFQTQ",
    "outputId": "6a0b7d8c-a5c0-4678-c5f1-1a849a3cb653"
   },
   "outputs": [],
   "source": [
    "# Weights of the second convolutional layer of the CNN - which was trained/optimized\n",
    "second_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1716478963693,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "oXsPH9ufFQTQ",
    "outputId": "3af5df7a-972c-4c28-d3e9-4a817e33b697"
   },
   "outputs": [],
   "source": [
    "# Biases of the second convolutional layer of the CNN - which was trained/optimized\n",
    "second_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1716478963693,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "ZrNh13GpFQTQ",
    "outputId": "fc812582-3d06-4437-a3e9-497b3ca4334b"
   },
   "outputs": [],
   "source": [
    "# Notice that we have indeed 4608 weights + 32 biases = 4640 parameters for the third convolutional layer\n",
    "3*3*16*32+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "AmXxCDaMFQTR",
    "outputId": "fc6b2ba1-4a33-4b1f-869a-50bca05e2cee"
   },
   "outputs": [],
   "source": [
    "# Accessing the third convolutional layer of the CNN\n",
    "third_convolutional_layer = model.layers[4]\n",
    "third_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "E5ipSxCaFQTR",
    "outputId": "78c5e81e-55ad-47c6-fe91-2fe3c60645a0"
   },
   "outputs": [],
   "source": [
    "# Weights of the third convolutional layer of the CNN - which was trained/optimized\n",
    "third_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "C09TEACQFQTR",
    "outputId": "ae44990b-ab16-454b-85ee-bc04a23588a0"
   },
   "outputs": [],
   "source": [
    "# Biases of the third convolutional layer of the CNN - which was trained/optimized\n",
    "third_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "dbaEj28UFQTR",
    "outputId": "afbd4087-7216-40f1-f201-c9a1df39416b"
   },
   "outputs": [],
   "source": [
    "# Notice that we have indeed 18432 weights + 64 biases = 18496 parameters for the fourth convolutional layer\n",
    "3*3*32*64+64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "7fUCmpttFQTR",
    "outputId": "897d9243-1a4e-42f4-87a0-08933e11598b"
   },
   "outputs": [],
   "source": [
    "# Accessing the fourth convolutional layer of the CNN\n",
    "fourth_convolutional_layer = model.layers[6]\n",
    "fourth_convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "uWHvBVJ1FQTR",
    "outputId": "6d6537bb-e1c1-4610-eb59-0c4af6496db5"
   },
   "outputs": [],
   "source": [
    "# Weights of the fourth convolutional layer of the CNN - which was trained/optimized\n",
    "fourth_convolutional_layer.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "W3OepQgyFQTR",
    "outputId": "78e07f8f-b64c-4e25-c375-0546374ff248"
   },
   "outputs": [],
   "source": [
    "# Biases of the fourth convolutional layer of the CNN - which was trained/optimized\n",
    "fourth_convolutional_layer.weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716478963694,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "ACVWUmqjFQTS",
    "outputId": "3afbe3a2-07fa-4a92-b5d9-43f56b892b99"
   },
   "outputs": [],
   "source": [
    "# Notice that we have indeed 16384 weights + 64 biases = 16448 parameters for the fourth convolutional layer\n",
    "(2*2*64+1)*64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9VRpl3JFQTS"
   },
   "source": [
    "üöÄ Now that we know how to access the trained weights of each kernel for every convolutional layer, we are going to investigate how they impact the analysis of images.\n",
    "\n",
    "* ü™Ñ Using **`plot_convolution(activation = True)`**, let's display the 16 kernels from the first convolutional layer, alongside with the activation output, to see what the model has learned from the images in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2767,
     "status": "ok",
     "timestamp": 1716478966456,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "yhTuTIgDFQTS",
    "outputId": "074aea53-e771-4e71-d6a3-5a00523b8b9d",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# First Convolutional Neural Network\n",
    "layer_number = 0\n",
    "\n",
    "# The input image has only 1 channel - Black&White picture\n",
    "channel_number = 0\n",
    "\n",
    "# Analyzing the impact of all the 16 kernels of the first convolutional layer\n",
    "for k in np.arange(0, 16):\n",
    "    print(f\"------------------------- Effect of the kernel number {k} -------------------------\")\n",
    "    kernel = model.layers[layer_number].weights[0].numpy()[:, :, channel_number, k]\n",
    "    plot_convolution(X[1], kernel, activation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fQf9DwgFQTS"
   },
   "source": [
    "#### (2.3.2) Activations üß®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXt6UD8xFQTS"
   },
   "source": [
    "We have been looking at the activation (\"_output image_\") only after the **first convolutional layer**.\n",
    "\n",
    "ü§î What if we want to **visualize the activation of an image after every convolutional layer of the CNN** ?\n",
    "\n",
    "* üìö We are going to use the [**Functional API**](https://www.tensorflow.org/guide/keras/functional) from Tensorflow/Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1EijIC5FQTS"
   },
   "source": [
    "***Step 0Ô∏è‚É£ : Reminders of the CNN's summary***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1716478966456,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "dcbHAtYpFQTS",
    "outputId": "56de5a39-c40d-47c2-c30f-a41ca92e55cc"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkC7YU78FQTT"
   },
   "source": [
    "***Step 1Ô∏è‚É£: listing all the 11 layers' outputs of our CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716478966456,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "O9HMobNkFQTT",
    "outputId": "0b901fbe-86f8-4278-ea1b-629122736677"
   },
   "outputs": [],
   "source": [
    "layers_outputs = [layer.output for layer in model.layers]\n",
    "layers_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW-W__qXFQTT"
   },
   "source": [
    "***Step 2Ô∏è‚É£ : Instantiate 11 sub-models re-using already trained weights and biases***\n",
    "- layer1\n",
    "- layer1 $ \\Rightarrow $ layer2\n",
    "- layer1 $ \\Rightarrow $ layer2 $ \\Rightarrow $ layer3\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_models = []\n",
    "for i in range(len(model.layers)):\n",
    "    # Get the first `i` layers from the trained model\n",
    "    layers_to_include = model.layers[:i+1]\n",
    "    # Build submodel\n",
    "    inputs = layers.Input(shape=(76, 78, 1)) # dummy input\n",
    "    x = inputs\n",
    "    for layer in layers_to_include:\n",
    "        x = layer(x)\n",
    "\n",
    "    # Compile again the sub-model to avoid error\n",
    "    submodel = models.Model(inputs, x)\n",
    "    submodel.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    activation_models.append(submodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-_LLLf8FQTT"
   },
   "source": [
    "***Step 3Ô∏è‚É£ : Compute the outputs of each submodel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1716478968722,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "heS-64F6FQTT",
    "outputId": "12ecfe1c-a9ec-46d6-e131-eeb9bd3eb87c"
   },
   "outputs": [],
   "source": [
    "activations = [m.predict(X) for m in activation_models]\n",
    "len(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1716478968722,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "g4RITctpFQTT",
    "outputId": "fd893b4e-be35-4833-fae7-a293ac000b1f"
   },
   "outputs": [],
   "source": [
    "[activation.shape for activation in activations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH9E1I2WFQTU"
   },
   "source": [
    "#### (2.3.3) The final show part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb8I3ZqdFQTU"
   },
   "source": [
    "Now that the activations are computed, we can choose one image in the dataset and observe the different \"activation images\" through each convolutional layer! In other terms, we are now able to observe what the CNN sees for each image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t3LVldpFQTU"
   },
   "source": [
    "* üëá Run the code down below and observe how a triangle was seen through the different convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18189,
     "status": "ok",
     "timestamp": 1716478987157,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "79ZMb25OFQTU",
    "outputId": "17f0aee3-fb9a-4047-a832-2172c4e348ca"
   },
   "outputs": [],
   "source": [
    "# Selecting a random triangle\n",
    "image_number = np.random.choice(np.where(y == 1)[0])\n",
    "\n",
    "for layer_number in [0,2,4,6]:\n",
    "\n",
    "    print(f\"--- Observing the effect of the convolutional layer number {layer_number}... ---\")\n",
    "    print(\"\")\n",
    "\n",
    "    temp_number_kernels = model.layers[layer_number].weights[0].shape[-1]\n",
    "    print(f\"{temp_number_kernels} kernels were applied and here are all the activations of this Conv2D Layer:\")\n",
    "\n",
    "    fig, axes = plt.subplots(int(temp_number_kernels/4),4, figsize=(20,7))\n",
    "\n",
    "\n",
    "    for ax, kernel_number in zip(axes.flat,range(temp_number_kernels)):\n",
    "        activation = activations[layer_number][image_number][:, :, kernel_number]\n",
    "        ax.imshow(activation, cmap=\"gray\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZmi6bvCFQTU"
   },
   "source": [
    "* üëá Run the code down below and observe how a circle) was seen through the different convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 17155,
     "status": "ok",
     "timestamp": 1716479004307,
     "user": {
      "displayName": "Aygul Zagidullina",
      "userId": "13153899368049053995"
     },
     "user_tz": -120
    },
    "id": "zbJTxOk7FQTU",
    "outputId": "18ebbef2-cefd-4d11-a249-c1282039055d"
   },
   "outputs": [],
   "source": [
    "# Selecting a random triangle\n",
    "image_number = np.random.choice(np.where(y == 0)[0])\n",
    "\n",
    "for layer_number in [0,2,4,6]:\n",
    "\n",
    "    print(f\"--- Observing the effect of the convolutional layer number {layer_number}... ---\")\n",
    "    print(\"\")\n",
    "\n",
    "    temp_number_kernels = model.layers[layer_number].weights[0].shape[-1]\n",
    "    print(f\"{temp_number_kernels} kernels were applied and here are all the activations of this Conv2D Layer:\")\n",
    "\n",
    "    fig, axes = plt.subplots(int(temp_number_kernels/4),4, figsize=(20,7))\n",
    "\n",
    "\n",
    "    for ax, kernel_number in zip(axes.flat,range(temp_number_kernels)):\n",
    "        activation = activations[layer_number][image_number][:, :, kernel_number]\n",
    "        ax.imshow(activation, cmap=\"gray\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwIM1tIIFQTU"
   },
   "source": [
    "üßëüèª‚Äçüè´ ***Notes*** üßëüèª‚Äçüè´\n",
    "\n",
    "1. Notice how the information of an image **flows** through the Convolutional Neural Network.\n",
    "2. You should see the picture becoming more and more \"abstract\", of smaller and smaller \"dimensions\"\n",
    "\n",
    "üïπ Feel free to play with the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) from researchers at [Georgia Tech](https://www.gatech.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOEU6953FQTV"
   },
   "source": [
    "## (Appendix) Utils\n",
    "\n",
    "* The following section simply presents the functions that were used to create the dataset with triangles and circles.\n",
    "\n",
    "* They are at the end of the notebook just in case you want to further prototype and get better understanding of what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3dzxeHIFQTV"
   },
   "outputs": [],
   "source": [
    "def draw_triangle():\n",
    "    dx = np.random.uniform(0.1, 0.3)\n",
    "    dy = np.random.uniform(0.1, 0.3)\n",
    "    noise_x = np.random.uniform(0.0, 0.1)\n",
    "    noise_y = np.random.uniform(0.0, 0.1)\n",
    "\n",
    "    x = np.random.uniform(0, 1-dx-noise_x)\n",
    "    y = np.random.uniform(0, 1-dy)\n",
    "    X = np.array([[x,y], [x+dx+noise_x,y], [x+dx/2, y+dy+noise_y]])\n",
    "\n",
    "    t1 = plt.Polygon(X, color='black')\n",
    "    plt.gca().add_patch(t1)\n",
    "\n",
    "def draw_circle():\n",
    "    r = np.random.uniform(0.1, 0.25)\n",
    "    x = np.random.uniform(0+r, 1-r)\n",
    "    y = np.random.uniform(0+r, 1-r)\n",
    "\n",
    "    circle1 = plt.Circle((x, y), r, color='black')\n",
    "    plt.gcf().gca().add_artist(circle1)\n",
    "\n",
    "def create_image(form, path):\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    if form == 'circle':\n",
    "        draw_circle()\n",
    "    elif form == 'triangle':\n",
    "        draw_triangle()\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, dpi=80, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_images(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "\n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        create_image('circle', c_path)\n",
    "\n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        create_image('triangle', t_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
