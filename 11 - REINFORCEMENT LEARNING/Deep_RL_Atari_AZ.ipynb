{"cells":[{"cell_type":"markdown","metadata":{"id":"k7xBVPzoXxOg"},"source":["# Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo (based on HuggingFace materials)\n","\n","In this notebook, **you'll train a Deep Q-Learning agent** playing Space Invaders using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n","\n","We're using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n","\n","‚¨áÔ∏è Here is an example of what **you will achieve** ‚¨áÔ∏è"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9S713biXntc"},"outputs":[],"source":["%%html\n","<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"]},{"cell_type":"markdown","source":["### üéÆ Environments:\n","\n","- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n","\n","You can see the difference between Space Invaders versions here üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n","\n","### üìö RL-Library:\n","\n","- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"],"metadata":{"id":"ykJiGevCMVc5"}},{"cell_type":"markdown","metadata":{"id":"wciHGjrFYz9m"},"source":["## Objectives of this notebook üèÜ\n","At the end of the notebook, you will:\n","- Be able to understand deeper **how RL Baselines3 Zoo works**.\n","- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QR0jZtYreSI5"},"source":["# Let's train a Deep Q-Learning agent playing Atari' Space Invaders üëæ and upload it to the Hub.\n","\n","We strongly recommend students **to use Google Colab for the hands-on exercises instead of running them on their personal computers**.\n","\n","By using Google Colab, **you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments**.\n","\n"]},{"cell_type":"markdown","source":["## An advice üí°\n","\n","We're going to **train for 90 minutes with 1M timesteps**.\n","\n","And if you want to train more such as 10 M timesteps, this will take about 9 hours, potentially resulting in Colab timing out. In that case, we recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."],"metadata":{"id":"Nc8BnyVEc3Ys"}},{"cell_type":"markdown","source":["## Set the GPU üí™\n","- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type / Hardware Accelerator`:  **Python / T4 GPU**\n"],"metadata":{"id":"PU4FVzaoM6fC"}},{"cell_type":"markdown","source":["# Install RL-Baselines3 Zoo and its dependencies üìö\n","\n","If you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **this is normal and it's not a critical error** there's a conflict of version. But the packages we need are installed."],"metadata":{"id":"wS_cVefO-aYg"}},{"cell_type":"code","source":["!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"],"metadata":{"id":"S1A_E4z3awa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get install swig cmake ffmpeg"],"metadata":{"id":"8_MllY6Om1eI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4S9mJiKg6SqC"},"source":["To be able to use Atari games in Gymnasium we need to install atari package. And accept-rom-license to download the rom files (games files)."]},{"cell_type":"code","source":["!pip install gymnasium[atari]\n","!pip install gymnasium[accept-rom-license]"],"metadata":{"id":"NsRP-lX1_2fC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create a virtual display üîΩ\n","\n","During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n","\n","Hence the following cell will install the librairies and create and run a virtual screen üñ•"],"metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV6wjQ7Be7p5"},"outputs":[],"source":["%%capture\n","!apt install python-opengl\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"BE5JWP5rQIKf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5iPgzluo9z-u"},"source":["## Train our Deep Q-Learning Agent to Play Space Invaders üëæ\n","\n","To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n","\n","1. Create a hyperparameter config file that will contain our training hyperparameters called `dqn.yml`.\n","\n","This is a template example:\n","\n","```\n","SpaceInvadersNoFrameskip-v4:\n","  env_wrapper:\n","    - stable_baselines3.common.atari_wrappers.AtariWrapper\n","  frame_stack: 4\n","  policy: 'CnnPolicy'\n","  n_timesteps: !!float 1e6\n","  buffer_size: 100000\n","  learning_rate: !!float 1e-4\n","  batch_size: 32\n","  learning_starts: 100000\n","  target_update_interval: 1000\n","  train_freq: 4\n","  gradient_steps: 1\n","  exploration_fraction: 0.1\n","  exploration_final_eps: 0.01\n","  # If True, you need to deactivate handle_timeout_termination\n","  # in the replay_buffer_kwargs\n","  optimize_memory_usage: False\n","```"]},{"cell_type":"markdown","metadata":{"id":"_VjblFSVDQOj"},"source":["Here we see that:\n","- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale, stack 4 frames)\n","- We use `CnnPolicy`, since we use Convolutional layers to process the frames\n","- We train it for 1 million `n_timesteps`\n","- Memory (Experience Replay) size is 100000, aka the amount of experience steps you saved to train again your agent with.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5qTkbWrkECOJ"},"source":["In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n","- `learning_rate`\n","- `buffer_size (Experience Memory size)`\n","- `batch_size`\n","\n","As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hn8bRTHvERRL"},"source":["2. We start the training and save the models on `logs` folder üìÅ\n","\n","- Define the algorithm after `--algo`, where we save the model after `-f` and where the hyperparameter config is after `-c`."]},{"cell_type":"markdown","metadata":{"id":"SeChoX-3SZfP"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuocgdokSab9"},"outputs":[],"source":["!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"]},{"cell_type":"markdown","metadata":{"id":"_dLomIiMKQaf"},"source":["## Let's evaluate our agent üëÄ\n","- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n","- Let's evaluate it for 5000 timesteps üî•"]},{"cell_type":"markdown","metadata":{"id":"Q24K1tyWSj7t"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_uSmwGRSk0z"},"outputs":[],"source":["!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"]},{"cell_type":"markdown","metadata":{"id":"liBeTltiHJtr"},"source":["## Publish our trained model on the Hub üöÄ\n","Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"]},{"cell_type":"markdown","metadata":{"id":"ezbHS1q3HYVV"},"source":["By using `rl_zoo3.push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n","\n","This way:\n","- You can **showcase our work** üî•\n","- You can **visualize your agent playing** üëÄ\n","- You can **share with the community an agent that others can use** üíæ\n"]},{"cell_type":"markdown","metadata":{"id":"XMSeZRBiHk6X"},"source":["To be able to share your model with the community there are three more steps to follow:\n","\n","1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n","\n","2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n","- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"]},{"cell_type":"markdown","metadata":{"id":"9O6FI0F8HnzE"},"source":["- Copy the token\n","- Run the cell below and past the token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ppu9yePwHrZX"},"outputs":[],"source":["from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n","notebook_login()\n","!git config --global credential.helper store"]},{"cell_type":"markdown","metadata":{"id":"dSLwdmvhHvjw"},"source":["3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî•"]},{"cell_type":"markdown","metadata":{"id":"PW436XnhHw1H"},"source":["Let's run push_to_hub.py file to upload our trained agent to the Hub.\n","\n","`--repo-name `: The name of the repo\n","\n","`-orga`: Your Hugging Face username\n","\n","`-f`: Where the trained model folder is (in our case `logs`)\n"]},{"cell_type":"markdown","metadata":{"id":"otgpa0rhS9wR"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HQNlAXuEhci"},"outputs":[],"source":["!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga aygul0790  -f logs/"]},{"cell_type":"markdown","metadata":{"id":"fyRKcCYY-dIo"},"source":["## Load a powerful trained model üî•\n","\n","- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n","\n","You can find them here: üëâ https://huggingface.co/sb3\n","\n","<br>\n","\n","Some examples:\n","- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n","- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n","- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n","- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n","\n","<br>\n","\n","Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-9QVFIROI5Y"},"outputs":[],"source":["%%html\n","<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"]},{"cell_type":"markdown","metadata":{"id":"7ZQNY_r6NJtC"},"source":["1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdBNZHy0NGTR"},"outputs":[],"source":["# Download model and save it into the logs/ folder\n","!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"]},{"cell_type":"markdown","metadata":{"id":"LFt6hmWsNdBo"},"source":["2. Let's evaluate if for 5000 timesteps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOxs0rNuN0uS"},"outputs":[],"source":["!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"]},{"cell_type":"markdown","metadata":{"id":"kxMDuDfPON57"},"source":["Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? üèÜ.**\n","\n","If you want to try, check https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in the model card, you have the hyperparameters of the trained agent.**"]},{"cell_type":"markdown","metadata":{"id":"xL_ZtUgpOuY6"},"source":["But finding hyperparameters can be a daunting task. Fortunately, we can **use Optuna for optimizing the Hyperparameters üî•.**\n"]},{"cell_type":"markdown","metadata":{"id":"-pqaco8W-huW"},"source":["## Some additional challenges üèÜ\n","The best way to learn **is to try things by your own**!\n","\n","\n","Here's a list of environments you can try to train your agent with:\n","- BeamRiderNoFrameskip-v4\n","- BreakoutNoFrameskip-v4\n","- EnduroNoFrameskip-v4\n","- PongNoFrameskip-v4\n","\n","\n","<br>\n","\n","\n","Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n","\n","<br>\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit3/unit3.ipynb","timestamp":1737133638650}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}